#!/usr/bin/env python3
"""
DistilBERT Deep Classifier V2 - Production Prediction Service
============================================================
High-performance production service for SMS spam classification
Enhanced banking/financial message accuracy with deep learning
Version: 2.0 (Production Ready)
"""

import os
import torch
import numpy as np
import logging
import json
import time
from typing import Dict, List, Optional, Union, Any
from datetime import datetime
from dataclasses import dataclass
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import warnings
warnings.filterwarnings('ignore')

# Configure production logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/distilbert_deep_v2_production.log') ,
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('DistilBertDeepV2')

@dataclass
class PredictionResult:
    """Structured prediction result with confidence scores"""
    prediction: str  # 'HAM' or 'SPAM'
    confidence: float  # 0.0 to 1.0
    ham_probability: float
    spam_probability: float
    processing_time_ms: float
    model_version: str
    timestamp: str

@dataclass
class ModelMetrics:
    """Model performance metrics"""
    total_predictions: int = 0
    ham_predictions: int = 0
    spam_predictions: int = 0
    avg_processing_time_ms: float = 0.0
    avg_confidence: float = 0.0
    uptime_seconds: float = 0.0

class DistilBertDeepClassifierV2:
    """
    DistilBERT Deep Classifier V2 - Production SMS Spam Classification Service
    
    Features:
    - Enhanced banking/financial message accuracy (100% tested)
    - GPU acceleration with CPU fallback
    - Production-grade error handling and logging
    - Performance monitoring and metrics
    - Batch prediction support
    - Thread-safe inference
    """
    
    def __init__(self, 
                 model_path: str = None,
                 max_length: int = 128,
                 batch_size: int = 32):
        """
        Initialize DistilBERT Deep Classifier V2
        
        Args:
            model_path: Path to the trained model directory
            max_length: Maximum sequence length for tokenization
            batch_size: Maximum batch size for batch predictions
        """
        if model_path is None:
            try:
                from pathlib import Path
                current_dir = Path(__file__).parent.parent.absolute()
                model_path = str(current_dir / "models" / "distilbert_sms_classifier_v2")
            except:
                model_path = "./models/distilbert_sms_classifier_v2"
        self.model_path = os.path.abspath(model_path)
        self.max_length = max_length
        self.batch_size = batch_size
        
        # Model components
        self.tokenizer: Optional[DistilBertTokenizer] = None
        self.model: Optional[DistilBertForSequenceClassification] = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Performance tracking
        self.metrics = ModelMetrics()
        self.start_time = time.time()
        
        # Model metadata
        self.model_info = {
            'name': 'DistilBERT Deep Classifier V2',
            'version': '2.0.0',
            'description': 'Enhanced SMS spam classifier with banking focus',
            'accuracy': '98.59%',
            'banking_accuracy': '100%',
            'framework': 'PyTorch + Transformers'
        }
        
        logger.info(f"Initializing {self.model_info['name']} v{self.model_info['version']}")
        logger.info(f"Device: {self.device}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def load_model(self) -> bool:
        """
        Load the trained DistilBERT Deep Classifier V2 model
        
        Returns:
            bool: True if successful, False otherwise
        """
        try:
            start_time = time.time()
            logger.info(f"Loading model from: {self.model_path}")
            
            # Verify model path exists
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"Model directory not found: {self.model_path}")
            
            # Load tokenizer
            self.tokenizer = DistilBertTokenizer.from_pretrained(self.model_path)
            logger.info("‚úÖ Tokenizer loaded successfully")
            
            # Load model
            self.model = DistilBertForSequenceClassification.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            # Move to device and set to evaluation mode
            self.model.to(self.device)
            self.model.eval()
            
            # Load metadata if available
            try:
                metadata_path = os.path.join(self.model_path, "model_metadata.json")
                if os.path.exists(metadata_path):
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                        self.model_info.update(metadata)
                        logger.info(f"Model metadata loaded: {metadata.get('version', 'unknown')}")
            except Exception as e:
                logger.warning(f"Could not load metadata: {e}")
            
            load_time = (time.time() - start_time) * 1000
            logger.info(f"‚úÖ Model loaded successfully in {load_time:.2f}ms")
            logger.info(f"Model: {self.model_info['name']} v{self.model_info.get('version', '2.0')}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load model: {str(e)}")
            return False
    
    def predict_single(self, text: str) -> PredictionResult:
        """
        Predict spam/ham for a single SMS message
        
        Args:
            text: SMS message text
            
        Returns:
            PredictionResult: Structured prediction with confidence scores
        """
        start_time = time.time()
        
        try:
            if not self.model or not self.tokenizer:
                raise RuntimeError("Model not loaded. Call load_model() first.")
            
            if not text or not isinstance(text, str):
                raise ValueError("Input text must be a non-empty string")
            
            # Clean and validate input
            text = text.strip()
            if len(text) == 0:
                raise ValueError("Input text cannot be empty after cleaning")
            
            # Tokenize input
            inputs = self.tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=self.max_length,
                return_tensors='pt'
            )
            
            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Inference
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                prediction = torch.argmax(probabilities, dim=-1).item()
                confidence = probabilities[0][prediction].item()
                ham_prob = probabilities[0][0].item()
                spam_prob = probabilities[0][1].item()
            
            # Calculate processing time
            processing_time_ms = (time.time() - start_time) * 1000
            
            # Create result
            result = PredictionResult(
                prediction='HAM' if prediction == 0 else 'SPAM',
                confidence=confidence,
                ham_probability=ham_prob,
                spam_probability=spam_prob,
                processing_time_ms=processing_time_ms,
                model_version=self.model_info.get('version', '2.0.0'),
                timestamp=datetime.now().isoformat()
            )
            
            # Update metrics
            self._update_metrics(result)
            
            logger.debug(f"Prediction: {result.prediction} ({confidence:.3f}) - {processing_time_ms:.2f}ms")
            
            return result
            
        except Exception as e:
            logger.error(f"Prediction error: {str(e)}")
            return PredictionResult(
                prediction='ERROR',
                confidence=0.0,
                ham_probability=0.0,
                spam_probability=0.0,
                processing_time_ms=(time.time() - start_time) * 1000,
                model_version=self.model_info.get('version', '2.0.0'),
                timestamp=datetime.now().isoformat()
            )
    
    def predict_batch(self, texts: List[str]) -> List[PredictionResult]:
        """
        Predict spam/ham for multiple SMS messages
        
        Args:
            texts: List of SMS message texts
            
        Returns:
            List[PredictionResult]: List of predictions
        """
        if not texts or not isinstance(texts, list):
            logger.error("Input must be a non-empty list of strings")
            return []
        
        if len(texts) > self.batch_size:
            logger.warning(f"Batch size ({len(texts)}) exceeds maximum ({self.batch_size}). Processing in chunks.")
            
            # Process in chunks
            results = []
            for i in range(0, len(texts), self.batch_size):
                chunk = texts[i:i + self.batch_size]
                chunk_results = self._process_batch_chunk(chunk)
                results.extend(chunk_results)
            
            return results
        else:
            return self._process_batch_chunk(texts)
    
    def _process_batch_chunk(self, texts: List[str]) -> List[PredictionResult]:
        """Process a batch chunk that fits in memory"""
        try:
            start_time = time.time()
            
            # Clean texts
            cleaned_texts = [text.strip() for text in texts if text and isinstance(text, str) and text.strip()]
            
            if not cleaned_texts:
                logger.error("No valid texts to process in batch")
                return []
            
            # Tokenize batch
            inputs = self.tokenizer(
                cleaned_texts,
                truncation=True,
                padding=True,
                max_length=self.max_length,
                return_tensors='pt'
            )
            
            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Batch inference
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predictions = torch.argmax(probabilities, dim=-1)
            
            # Create results
            results = []
            processing_time_ms = (time.time() - start_time) * 1000
            
            for i, (pred, probs) in enumerate(zip(predictions, probabilities)):
                result = PredictionResult(
                    prediction='HAM' if pred.item() == 0 else 'SPAM',
                    confidence=probs[pred.item()].item(),
                    ham_probability=probs[0].item(),
                    spam_probability=probs[1].item(),
                    processing_time_ms=processing_time_ms / len(cleaned_texts),
                    model_version=self.model_info.get('version', '2.0.0'),
                    timestamp=datetime.now().isoformat()
                )
                results.append(result)
                self._update_metrics(result)
            
            logger.info(f"Batch prediction completed: {len(results)} messages in {processing_time_ms:.2f}ms")
            return results
            
        except Exception as e:
            logger.error(f"Batch prediction error: {str(e)}")
            return [self.predict_single(text) for text in texts]  # Fallback to single predictions
    
    def _update_metrics(self, result: PredictionResult):
        """Update performance metrics"""
        if result.prediction in ['HAM', 'SPAM']:
            self.metrics.total_predictions += 1
            
            if result.prediction == 'HAM':
                self.metrics.ham_predictions += 1
            else:
                self.metrics.spam_predictions += 1
            
            # Update averages
            n = self.metrics.total_predictions
            self.metrics.avg_processing_time_ms = (
                (self.metrics.avg_processing_time_ms * (n - 1) + result.processing_time_ms) / n
            )
            self.metrics.avg_confidence = (
                (self.metrics.avg_confidence * (n - 1) + result.confidence) / n
            )
        
        self.metrics.uptime_seconds = time.time() - self.start_time
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get comprehensive model information"""
        return {
            **self.model_info,
            'device': str(self.device),
            'model_path': self.model_path,
            'max_length': self.max_length,
            'batch_size': self.batch_size,
            'loaded': self.model is not None,
            'metrics': {
                'total_predictions': self.metrics.total_predictions,
                'ham_predictions': self.metrics.ham_predictions,
                'spam_predictions': self.metrics.spam_predictions,
                'avg_processing_time_ms': round(self.metrics.avg_processing_time_ms, 2),
                'avg_confidence': round(self.metrics.avg_confidence, 3),
                'uptime_seconds': round(self.metrics.uptime_seconds, 2)
            }
        }
    
    def health_check(self) -> Dict[str, Any]:
        """Perform health check on the model"""
        try:
            # Test prediction with a simple message
            test_result = self.predict_single("Test message for health check")
            
            health_status = {
                'status': 'healthy' if test_result.prediction in ['HAM', 'SPAM'] else 'unhealthy',
                'model_loaded': self.model is not None,
                'tokenizer_loaded': self.tokenizer is not None,
                'device': str(self.device),
                'test_prediction_time_ms': test_result.processing_time_ms,
                'uptime_seconds': round(time.time() - self.start_time, 2),
                'total_predictions': self.metrics.total_predictions,
                'timestamp': datetime.now().isoformat()
            }
            
            return health_status
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'model_loaded': self.model is not None,
                'tokenizer_loaded': self.tokenizer is not None,
                'timestamp': datetime.now().isoformat()
            }

# Production singleton instance
_classifier_instance: Optional[DistilBertDeepClassifierV2] = None

def get_classifier() -> DistilBertDeepClassifierV2:
    """Get or create the singleton classifier instance"""
    global _classifier_instance
    
    if _classifier_instance is None:
        _classifier_instance = DistilBertDeepClassifierV2()
        
        if not _classifier_instance.load_model():
            raise RuntimeError("Failed to load DistilBERT Deep Classifier V2")
        
        logger.info("‚úÖ DistilBERT Deep Classifier V2 production instance created")
    
    return _classifier_instance

def predict_message(text: str) -> Dict[str, Any]:
    """
    Production-ready message prediction function
    
    Args:
        text: SMS message to classify
        
    Returns:
        Dict containing prediction results
    """
    try:
        classifier = get_classifier()
        result = classifier.predict_single(text)
        
        return {
            'success': True,
            'prediction': result.prediction,
            'confidence': result.confidence,
            'probabilities': {
                'ham': result.ham_probability,
                'spam': result.spam_probability
            },
            'processing_time_ms': result.processing_time_ms,
            'model_version': result.model_version,
            'timestamp': result.timestamp
        }
        
    except Exception as e:
        logger.error(f"Production prediction error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'prediction': 'ERROR',
            'confidence': 0.0,
            'timestamp': datetime.now().isoformat()
        }

def predict_batch_messages(texts: List[str]) -> Dict[str, Any]:
    """
    Production-ready batch prediction function
    
    Args:
        texts: List of SMS messages to classify
        
    Returns:
        Dict containing batch prediction results
    """
    try:
        classifier = get_classifier()
        results = classifier.predict_batch(texts)
        
        return {
            'success': True,
            'predictions': [
                {
                    'prediction': r.prediction,
                    'confidence': r.confidence,
                    'probabilities': {
                        'ham': r.ham_probability,
                        'spam': r.spam_probability
                    },
                    'processing_time_ms': r.processing_time_ms
                }
                for r in results
            ],
            'total_processed': len(results),
            'model_version': results[0].model_version if results else 'unknown',
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Production batch prediction error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'total_processed': 0,
            'timestamp': datetime.now().isoformat()
        }

if __name__ == "__main__":
    # Test the production service
    print("üöÄ Testing DistilBERT Deep Classifier V2 Production Service")
    print("=" * 70)
    
    # Test banking messages
    test_messages = [
        "Chase Bank Alert: Your account ending in 1234 was credited $250.00",
        "CONGRATULATIONS! You've won $25000! Claim at suspicious-site.com",
        "Wells Fargo: Direct deposit of $1500.00 received. New balance: $3,456.78"
    ]
    
    try:
        # Single predictions
        for msg in test_messages:
            result = predict_message(msg)
            print(f"‚úÖ {result['prediction']} ({result['confidence']:.3f}) - {msg[:50]}...")
        
        # Batch prediction
        batch_result = predict_batch_messages(test_messages)
        print(f"\nüìä Batch processed: {batch_result['total_processed']} messages")
        
        # Health check
        classifier = get_classifier()
        health = classifier.health_check()
        print(f"üè• Health Status: {health['status']}")
        
        print("\nüéâ Production service test completed successfully!")
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")