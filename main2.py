#!/usr/bin/env python3
"""
SMS SPAM DETECTION API - Environment Compatible Version
Fixed for cross-platform deployment and dependency issues
"""

import os
import sys
import logging
from pathlib import Path
import warnings

# Suppress transformers warnings early
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers")

# Configure logging first
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set environment variables for better compatibility
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['TRANSFORMERS_OFFLINE'] = '0'
os.environ['HF_HOME'] = str(Path.cwd() / 'models' / '.cache')

# Try to download models with better error handling
def safe_model_download():
    """Safely attempt model download with fallback"""
    try:
        from model_downloader import downloader
        logger.info("üîÑ Attempting model download...")
        success = downloader.download_all_models()
        if success:
            logger.info("‚úÖ Model download completed")
        else:
            logger.warning("‚ö†Ô∏è Some models failed to download - proceeding with available models")
        return success
    except ImportError:
        logger.warning("‚ö†Ô∏è Model downloader not available - using existing models")
        return False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Model download failed: {e} - using existing models")
        return False

# Attempt model download
safe_model_download()

# Core imports
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager
import pickle
from typing import Optional, Dict, Any, Union, List
import numpy as np
import re
import time

# Import all preprocessor classes for pickle compatibility
class FastSMSPreprocessor:
    """Original SMS preprocessor for backward compatibility"""
    
    def __init__(self):
        self.sms_abbreviations = {
            'u': 'you', 'ur': 'your', 'r': 'are', '2': 'to', '4': 'for',
            'txt': 'text', 'msg': 'message', 'pls': 'please', 'thx': 'thanks',
            'asap': 'as soon as possible', 'fyi': 'for your information',
            'im': 'i am', 'ive': 'i have', 'dont': 'do not'
        }
    
    def normalize_text(self, text):
        """Fast text normalization"""
        if not isinstance(text, str):
            return ""
        
        text = text.lower()
        
        # Quick normalizations
        text = re.sub(r'\b\d{10,}\b', 'PHONE', text)
        text = re.sub(r'http\S+|www\.\S+', 'URL', text)
        text = re.sub(r'[¬£$‚Ç¨]\s*\d+', 'MONEY', text)
        text = re.sub(r'\b\d{5,6}\b', 'CODE', text)
        
        # Expand abbreviations
        words = text.split()
        for i, word in enumerate(words):
            clean_word = word.strip('.,!?')
            if clean_word in self.sms_abbreviations:
                words[i] = self.sms_abbreviations[clean_word]
        
        return ' '.join(words)

class SimplePreprocessor:
    """Simple preprocessor for maximum compatibility"""
    def normalize_text(self, text):
        if not isinstance(text, str):
            return ""
        text = text.lower().strip()
        text = re.sub(r'http\S+|www\.\S+', 'URL', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def extract_features(self, text):
        """Extract basic numerical features"""
        if not isinstance(text, str):
            return np.zeros(30)
        
        features = [
            len(text),  # length
            len(text.split()),  # word count
            text.count('!'),  # exclamation marks
            text.count('?'),  # question marks
            text.count('$') + text.count('¬£') + text.count('‚Ç¨'),  # currency
            len(re.findall(r'\d+', text)),  # numbers
            sum(1 for c in text if c.isupper()) / max(len(text), 1),  # uppercase ratio
        ]
        
        # Pad to 30 features
        features.extend([0.0] * (30 - len(features)))
        return np.array(features[:30], dtype=float)

# Register classes globally for pickle compatibility
def register_classes_globally():
    """Register all preprocessor classes for pickle compatibility"""
    import __main__
    
    classes = {
        'FastSMSPreprocessor': FastSMSPreprocessor,
        'SimplePreprocessor': SimplePreprocessor,
    }
    
    for name, cls in classes.items():
        setattr(__main__, name, cls)
        globals()[name] = cls
    
    # Register module in sys.modules
    sys.modules['main'] = sys.modules[__name__]
    logger.info("‚úÖ Preprocessor classes registered for pickle compatibility")

register_classes_globally()

# Safe imports with fallbacks
def safe_import_transformers():
    """Safely import transformers with fallback"""
    try:
        import torch
        # Set torch to CPU only to avoid GPU issues on different environments
        torch.set_num_threads(1)
        
        from transformers import (
            RobertaTokenizer, 
            RobertaForSequenceClassification,
            logging as transformers_logging
        )
        
        # Disable transformers warnings
        transformers_logging.set_verbosity_error()
        
        logger.info("‚úÖ Transformers library loaded successfully")
        return True, torch, RobertaTokenizer, RobertaForSequenceClassification
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Transformers not available: {e}")
        return False, None, None, None

# Import transformers safely
TRANSFORMERS_AVAILABLE, torch, RobertaTokenizer, RobertaForSequenceClassification = safe_import_transformers()

def safe_import_distilbert():
    """Safely import DistilBERT module"""
    try:
        # Try multiple import paths
        try:
            from backend.distilbert_deep_classifier_v2 import get_classifier, predict_message, predict_batch_messages
        except ImportError:
            from distilbert_deep_classifier_v2 import get_classifier, predict_message, predict_batch_messages
        
        logger.info("‚úÖ DistilBERT Deep Classifier V2 available")
        return True, predict_message, predict_batch_messages
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è DistilBERT Deep Classifier V2 not available: {e}")
        return False, None, None

DISTILBERT_AVAILABLE, predict_message, predict_batch_messages = safe_import_distilbert()

def safe_import_sklearn():
    """Safely import scikit-learn components"""
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.svm import SVC
        logger.info("‚úÖ Scikit-learn available")
        return True, TfidfVectorizer, SVC
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Scikit-learn not available: {e}")
        return False, None, None

SKLEARN_AVAILABLE, TfidfVectorizer, SVC = safe_import_sklearn()

class ModelManager:
    """Enhanced Model Manager with better error handling and fallbacks"""
    
    def __init__(self):
        self.models = {}
        self.load_all_models()

    def load_all_models(self):
        """Load all available models with comprehensive fallbacks"""
        logger.info("üîÑ Loading AI models...")
        
        # Load models in order of preference
        self._load_roberta_model()
        self._load_distilbert_model()
        self._load_xgboost_model()
        self._load_svm_model()
        
        # Ensure at least one working model
        working_models = [k for k, v in self.models.items() if v.get('model') and v['model'] != 'mock']
        if not working_models:
            logger.warning("‚ö†Ô∏è No ML models loaded, creating rule-based classifier")
            self._create_fallback_model()
            working_models = ['smart_rules']
        
        logger.info(f"‚úÖ Ready with {len(working_models)} working model(s): {working_models}")

    def _load_roberta_model(self):
        """Load RoBERTa model with safe error handling"""
        if not TRANSFORMERS_AVAILABLE:
            logger.warning("‚ö†Ô∏è RoBERTa not available - transformers library missing")
            self.models['roberta'] = self._create_mock_model('RoBERTa Ultimate Spam Detector', 99.72, 100000, 'roberta')
            return False
        
        try:
            roberta_path = self._get_model_path("roberta_spam_detector.pkl")
            
            if not os.path.exists(roberta_path):
                logger.warning(f"‚ö†Ô∏è RoBERTa model not found at {roberta_path}")
                self.models['roberta'] = self._create_mock_model('RoBERTa Ultimate Spam Detector', 99.72, 100000, 'roberta')
                return False
            
            with open(roberta_path, 'rb') as f:
                model_data = pickle.load(f)
            
            model = model_data['model']
            tokenizer = model_data['tokenizer']
            
            # Force CPU usage for compatibility
            device = torch.device('cpu')
            model.to(device)
            model.eval()
            
            self.models['roberta'] = {
                'model': model,
                'tokenizer': tokenizer,
                'device': device,
                'accuracy': model_data.get('accuracy', 99.72),
                'dataset_size': model_data.get('dataset_size', 100000),
                'name': 'RoBERTa Ultimate Spam Detector',
                'training_focus': 'State-of-the-art transformer-based spam detection',
                'type': 'roberta'
            }
            
            logger.info("‚úÖ RoBERTa model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading RoBERTa model: {e}")
            self.models['roberta'] = self._create_mock_model('RoBERTa Ultimate Spam Detector', 99.72, 100000, 'roberta')
            return False

    def _load_distilbert_model(self):
        """Load DistilBERT model with safe error handling"""
        if not DISTILBERT_AVAILABLE:
            logger.warning("‚ö†Ô∏è DistilBERT not available")
            self.models['distilbert_v2'] = self._create_mock_model('DistilBERT Deep Classifier V2', 98.59, 4962, 'distilbert_v2')
            return False
        
        try:
            distilbert_path = self._get_model_path("distilbert_deep_classifier_v2.pkl")
            
            if not os.path.exists(distilbert_path):
                logger.warning(f"‚ö†Ô∏è DistilBERT model not found at {distilbert_path}")
                self.models['distilbert_v2'] = self._create_mock_model('DistilBERT Deep Classifier V2', 98.59, 4962, 'distilbert_v2')
                return False
            
            with open(distilbert_path, 'rb') as f:
                model_data = pickle.load(f)
            
            if isinstance(model_data, dict) and 'model' in model_data:
                classifier = model_data['model']
            else:
                classifier = model_data
            
            # Ensure CPU usage
            if hasattr(classifier, 'to'):
                classifier = classifier.to('cpu')
            
            self.models['distilbert_v2'] = {
                'model': classifier,
                'accuracy': model_data.get('accuracy', 98.59) if isinstance(model_data, dict) else 98.59,
                'dataset_size': model_data.get('dataset_size', 4962) if isinstance(model_data, dict) else 4962,
                'name': 'DistilBERT Deep Classifier V2',
                'training_focus': 'Enhanced banking/financial message accuracy',
                'type': 'distilbert_v2'
            }
            
            logger.info("‚úÖ DistilBERT V2 model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading DistilBERT model: {e}")
            self.models['distilbert_v2'] = self._create_mock_model('DistilBERT Deep Classifier V2', 98.59, 4962, 'distilbert_v2')
            return False

    def _load_xgboost_model(self):
        """Load XGBoost model with safe error handling"""
        try:
            xgboost_path = self._get_model_path("xgboost_spam_detector.pkl")
            
            if not os.path.exists(xgboost_path):
                logger.warning(f"‚ö†Ô∏è XGBoost model not found at {xgboost_path}")
                self.models['xgboost'] = self._create_mock_model('XGBoost High-Accuracy', 98.13, 5885, 'xgboost')
                return False
            
            with open(xgboost_path, 'rb') as f:
                xgb_data = pickle.load(f)
            
            # Create fitted vectorizer if needed
            vectorizer = xgb_data.get('vectorizer')
            if not vectorizer or not hasattr(vectorizer, 'idf_') or vectorizer.idf_ is None:
                logger.warning("Creating new fitted vectorizer for XGBoost")
                vectorizer = self._create_fitted_vectorizer(10000)
            
            # Use simple preprocessor for compatibility
            preprocessor = xgb_data.get('preprocessor', SimplePreprocessor())
            if not hasattr(preprocessor, 'normalize_text'):
                preprocessor = SimplePreprocessor()
            
            self.models['xgboost'] = {
                'model': xgb_data['model'],
                'vectorizer': vectorizer,
                'preprocessor': preprocessor,
                'accuracy': 98.13,
                'dataset_size': 5885,
                'name': 'XGBoost High-Accuracy',
                'training_focus': 'Comprehensive spam detection',
                'type': 'xgboost'
            }
            
            logger.info("‚úÖ XGBoost model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading XGBoost model: {e}")
            self.models['xgboost'] = self._create_mock_model('XGBoost High-Accuracy', 98.13, 5885, 'xgboost')
            return False

    def _load_svm_model(self):
        """Load SVM model with safe error handling"""
        if not SKLEARN_AVAILABLE:
            logger.warning("‚ö†Ô∏è SVM not available - scikit-learn missing")
            self.models['svm'] = self._create_mock_model('SVM (Support Vector Machine)', 95.0, 2940, 'svm')
            return False
        
        try:
            svm_path = self._get_model_path("svm_spam_detector.pkl")
            
            if not os.path.exists(svm_path):
                logger.warning(f"‚ö†Ô∏è SVM model not found at {svm_path}")
                self.models['svm'] = self._create_mock_model('SVM (Support Vector Machine)', 95.0, 2940, 'svm')
                return False
            
            with open(svm_path, 'rb') as f:
                svm_data = pickle.load(f)
            
            # Create fitted vectorizer if needed
            vectorizer = svm_data.get('vectorizer')
            if not vectorizer or not hasattr(vectorizer, 'idf_') or vectorizer.idf_ is None:
                logger.warning("Creating new fitted vectorizer for SVM")
                vectorizer = self._create_fitted_vectorizer(859)
            
            # Use simple preprocessor for compatibility
            preprocessor = svm_data.get('preprocessor', SimplePreprocessor())
            if not hasattr(preprocessor, 'normalize_text'):
                preprocessor = SimplePreprocessor()
            
            self.models['svm'] = {
                'model': svm_data['model'],
                'vectorizer': vectorizer,
                'preprocessor': preprocessor,
                'accuracy': 95.0,
                'dataset_size': 2940,
                'name': 'SVM (Support Vector Machine)',
                'training_focus': 'Fast baseline spam detection',
                'type': 'svm'
            }
            
            logger.info("‚úÖ SVM model loaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error loading SVM model: {e}")
            self.models['svm'] = self._create_mock_model('SVM (Support Vector Machine)', 95.0, 2940, 'svm')
            return False

    def _get_model_path(self, filename):
        """Get model path with fallbacks for different environments"""
        possible_paths = [
            Path.cwd() / 'models' / filename,
            Path.cwd() / filename,
            Path.cwd() / 'backend' / 'models' / filename,
            Path(__file__).parent / 'models' / filename,
        ]
        
        for path in possible_paths:
            if path.exists():
                return str(path)
        
        # Return the most likely path even if it doesn't exist
        return str(Path.cwd() / 'models' / filename)

    def _create_fitted_vectorizer(self, max_features=5000):
        """Create a fitted TF-IDF vectorizer"""
        if not SKLEARN_AVAILABLE:
            return None
        
        # Sample training texts
        sample_texts = [
            "free money win prize urgent call now",
            "congratulations winner selected lottery cash",
            "urgent account suspended verify immediately",
            "limited time offer discount expires today",
            "hey running late start meeting without me",
            "can you pick up milk on way home",
            "meeting moved tomorrow conference room",
            "package delivered thank you for choosing",
            "otp verification code for bank account",
            "balance credited to your account successfully"
        ] * 20  # Repeat to create more vocabulary
        
        try:
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.9,
                stop_words=None,
                lowercase=True
            )
            
            vectorizer.fit(sample_texts)
            logger.info(f"Created fitted vectorizer with {len(vectorizer.vocabulary_)} features")
            return vectorizer
        except Exception as e:
            logger.error(f"Error creating vectorizer: {e}")
            return None

    def _create_mock_model(self, name: str, accuracy: float, dataset_size: int, model_type: str):
        """Create a mock model entry"""
        return {
            'model': 'mock',
            'accuracy': accuracy,
            'dataset_size': dataset_size,
            'name': name,
            'training_focus': 'Model not available - using fallback',
            'type': model_type
        }

    def _create_fallback_model(self):
        """Create intelligent rule-based fallback"""
        self.models['smart_rules'] = {
            'model': 'smart_classifier',
            'accuracy': 78.0,
            'dataset_size': 0,
            'name': 'Smart Rule-Based Classifier',
            'training_focus': 'Pattern-based spam detection',
            'type': 'rule_based'
        }

    def predict(self, message: str, model_name: str = 'roberta') -> Dict[str, Any]:
        """Make prediction with comprehensive fallback chain"""
        # Find best available model
        if model_name not in self.models or not self._is_model_working(model_name):
            for preferred_model in ['roberta', 'distilbert_v2', 'xgboost', 'svm', 'smart_rules']:
                if preferred_model in self.models and self._is_model_working(preferred_model):
                    model_name = preferred_model
                    break
        
        try:
            model_data = self.models[model_name]
            
            if model_name == 'roberta' and self._is_model_working('roberta'):
                return self._predict_roberta(message, model_data)
            elif model_name == 'distilbert_v2' and self._is_model_working('distilbert_v2'):
                return self._predict_distilbert_v2(message, model_data)
            elif model_name == 'xgboost' and self._is_model_working('xgboost'):
                return self._predict_xgboost(message, model_data)
            elif model_name == 'svm' and self._is_model_working('svm'):
                return self._predict_svm(message, model_data)
            else:
                return self._predict_smart_rules(message, model_data)
                
        except Exception as e:
            logger.error(f"Prediction error with {model_name}: {e}")
            # Final fallback
            return self._predict_smart_rules(message, self.models.get('smart_rules', {}))

    def _is_model_working(self, model_name: str) -> bool:
        """Check if model is actually working"""
        if model_name not in self.models:
            return False
        model = self.models[model_name].get('model')
        return model and model not in ['mock', 'smart_classifier']

    def _predict_roberta(self, message: str, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """RoBERTa prediction with error handling"""
        try:
            model = model_data['model']
            tokenizer = model_data['tokenizer']
            device = model_data['device']
            
            inputs = tokenizer(
                message,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            )
            
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probabilities = torch.nn.functional.softmax(logits, dim=-1)
            
            predicted_class = torch.argmax(logits, dim=-1).item()
            confidence = float(probabilities[0][predicted_class])
            
            return {
                'prediction': 'SPAM' if predicted_class == 1 else 'HAM',
                'confidence': confidence,
                'model_name': model_data['name'],
                'processed_text': message
            }
            
        except Exception as e:
            logger.error(f"RoBERTa prediction error: {e}")
            return self._predict_smart_rules(message, model_data)

    def _predict_distilbert_v2(self, message: str, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """DistilBERT prediction with error handling"""
        try:
            model = model_data.get('model')
            if model and hasattr(model, 'predict'):
                result = model.predict(message)
                
                if isinstance(result, dict):
                    return {
                        'prediction': result.get('prediction', 'HAM').upper(),
                        'confidence': result.get('confidence', 0.5),
                        'model_name': model_data['name'],
                        'processed_text': message
                    }
            
            # Fallback to module function if available
            if DISTILBERT_AVAILABLE and predict_message:
                result = predict_message(message)
                if result.get('success'):
                    return {
                        'prediction': result['prediction'].upper(),
                        'confidence': result['confidence'],
                        'model_name': model_data['name'],
                        'processed_text': message
                    }
            
            return self._predict_smart_rules(message, model_data)
            
        except Exception as e:
            logger.error(f"DistilBERT prediction error: {e}")
            return self._predict_smart_rules(message, model_data)

    def _predict_xgboost(self, message: str, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """XGBoost prediction with error handling"""
        try:
            preprocessor = model_data.get('preprocessor', SimplePreprocessor())
            processed = preprocessor.normalize_text(message)
            
            vectorizer = model_data['vectorizer']
            if not vectorizer:
                return self._predict_smart_rules(message, model_data)
            
            tfidf_features = vectorizer.transform([processed]).toarray()
            
            # Add numerical features if available
            if hasattr(preprocessor, 'extract_features'):
                try:
                    numerical_features = preprocessor.extract_features(message).reshape(1, -1)
                    features = np.hstack([tfidf_features, numerical_features])
                except:
                    features = tfidf_features
            else:
                features = tfidf_features
            
            model = model_data['model']
            prediction = model.predict(features)[0]
            
            try:
                proba = model.predict_proba(features)[0]
                confidence = float(max(proba))
            except:
                confidence = 0.85
            
            return {
                'prediction': 'SPAM' if prediction == 1 else 'HAM',
                'confidence': confidence,
                'model_name': model_data['name'],
                'processed_text': processed
            }
            
        except Exception as e:
            logger.error(f"XGBoost prediction error: {e}")
            return self._predict_smart_rules(message, model_data)

    def _predict_svm(self, message: str, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """SVM prediction with error handling"""
        try:
            preprocessor = model_data.get('preprocessor', SimplePreprocessor())
            processed = preprocessor.normalize_text(message)
            
            vectorizer = model_data['vectorizer']
            if not vectorizer:
                return self._predict_smart_rules(message, model_data)
            
            features = vectorizer.transform([processed])
            model = model_data['model']
            
            prediction = model.predict(features)[0]
            
            try:
                if hasattr(model, 'predict_proba'):
                    proba = model.predict_proba(features)[0]
                    confidence = float(max(proba))
                else:
                    confidence = 0.80
            except:
                confidence = 0.80
            
            return {
                'prediction': 'SPAM' if prediction == 1 else 'HAM',
                'confidence': confidence,
                'model_name': model_data['name'],
                'processed_text': processed
            }
            
        except Exception as e:
            logger.error(f"SVM prediction error: {e}")
            return self._predict_smart_rules(message, model_data)

    def _predict_smart_rules(self, message: str, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """Smart rule-based prediction as ultimate fallback"""
        processed = message.lower()
        
        spam_keywords = [
            'free', 'win', 'winner', 'prize', 'congratulations', 'urgent', 'call now',
            'claim', 'limited time', 'act now', 'guaranteed', 'offer expires',
            'click here', 'verify account', 'suspended', 'blocked', 'payment failed',
            'refund', 'lottery', 'selected', 'cash', 'money', 'discount'
        ]
        
        spam_count = sum(1 for word in spam_keywords if word in processed)
        
        # Pattern checks
        has_money = any(symbol in message for symbol in ['$', '¬£', '‚Ç¨', 'money', 'cash'])
        has_urgency = any(word in processed for word in ['urgent', 'immediately', 'now', 'expires'])
        has_link = any(pattern in processed for pattern in ['http', 'www.', '.com', 'click'])
        has_phone = bool(re.search(r'\b\d{10,}\b', message))
        has_caps = sum(1 for c in message if c.isupper()) / max(len(message), 1) > 0.3
        
        # Scoring
        score = spam_count * 2
        if has_money: score += 3
        if has_urgency: score += 2
        if has_link: score += 2
        if has_phone: score += 1
        if has_caps: score += 1
        
        prediction = 1 if score >= 4 else 0
        confidence = min(0.50 + (score * 0.06), 0.88)
        
        return {
            'prediction': 'SPAM' if prediction == 1 else 'HAM',
            'confidence': float(confidence),
            'model_name': model_data.get('name', 'Smart Rule-Based Classifier'),
            'processed_text': processed
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """Get available working models"""
        model_info = {}
        for key, model_data in self.models.items():
            if self._is_model_working(key):
                model_info[key] = {
                    'name': model_data['name'],
                    'accuracy': model_data['accuracy'],
                    'dataset_size': model_data['dataset_size'],
                    'training_focus': model_data['training_focus']
                }
        return {'available_models': model_info}

# Pydantic models for API
class SMSMessage(BaseModel):
    text: str
    model: Optional[str] = 'roberta'

class PredictionResponse(BaseModel):
    prediction: str
    confidence: float
    model_name: str
    processed_text: str
    risk_level: str
    message: str

# Enhanced endpoint for all 4 models
class AllModelsResponse(BaseModel):
    models: Dict[str, Dict[str, Any]]
    message: str
    total_models: int

# Global model manager
model_manager = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    global model_manager
    try:
        logger.info("üöÄ Initializing SMS Spam Detection API...")
        model_manager = ModelManager()
        logger.info("‚úÖ SMS Spam Detection API ready!")
        yield
    except Exception as e:
        logger.error(f"‚ùå Initialization failed: {e}")
        raise
    finally:
        logger.info("üîÑ Shutting down SMS Spam Detection API")

# FastAPI app
app = FastAPI(
    title="SMS Spam Detection API - Environment Compatible",
    description="Cross-platform AI SMS spam detection with multiple model support",
    version="5.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API Endpoints
@app.get("/")
async def root():
    """Root endpoint with API information"""
    if not model_manager:
        raise HTTPException(status_code=503, detail="API not ready")
    
    working_models = len([k for k in model_manager.models.keys() if model_manager._is_model_working(k)])
    
    return {
        "message": "SMS Spam Detection API - Environment Compatible",
        "version": "5.0.0",
        "models_loaded": working_models,
        "status": "operational",
        "available_endpoints": ["/predict", "/predict/all", "/models", "/health"]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    if not model_manager:
        raise HTTPException(status_code=503, detail="API not ready")
    
    working_models = [k for k in model_manager.models.keys() if model_manager._is_model_working(k)]
    
    return {
        "status": "healthy",
        "models_loaded": len(working_models),
        "working_models": working_models,
        "message": "All systems operational"
    }

@app.get("/models")
async def get_models():
    """Get available models"""
    if not model_manager:
        raise HTTPException(status_code=503, detail="Models not initialized")
    
    return model_manager.get_available_models()

@app.post("/predict", response_model=PredictionResponse)
async def predict_sms(sms: SMSMessage):
    """Predict if SMS is spam or ham - Single model"""
    if not model_manager:
        raise HTTPException(status_code=503, detail="Models not initialized")
    
    if not sms.text.strip():
        raise HTTPException(status_code=400, detail="SMS text cannot be empty")
    
    try:
        result = model_manager.predict(sms.text.strip(), sms.model)
        
        # Calculate risk level
        prediction = result['prediction']
        confidence = result['confidence']
        
        if prediction == 'SPAM':
            risk_level = "HIGH RISK" if confidence > 0.8 else "MEDIUM RISK" if confidence > 0.6 else "LOW RISK"
        else:
            risk_level = "SAFE" if confidence > 0.8 else "LIKELY SAFE" if confidence > 0.6 else "UNCERTAIN"
        
        return PredictionResponse(
            prediction=prediction,
            confidence=confidence,
            model_name=result['model_name'],
            processed_text=result['processed_text'],
            risk_level=risk_level,
            message=sms.text.strip()
        )
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail="Internal prediction error")

@app.post("/predict/all", response_model=AllModelsResponse)
async def predict_all_models(sms: SMSMessage):
    """Predict using all available models - For frontend all-models display"""
    if not model_manager:
        raise HTTPException(status_code=503, detail="Models not initialized")
    
    if not sms.text.strip():
        raise HTTPException(status_code=400, detail="SMS text cannot be empty")
    
    try:
        all_results = {}
        model_names = ['roberta', 'distilbert_v2', 'xgboost', 'svm']
        
        for model_name in model_names:
            try:
                result = model_manager.predict(sms.text.strip(), model_name)
                
                # Calculate risk level and confidence percentage
                prediction = result['prediction']
                confidence = result['confidence']
                confidence_percentage = round(confidence * 100, 1)
                
                if prediction == 'SPAM':
                    risk_level = "HIGH RISK" if confidence > 0.8 else "MEDIUM RISK" if confidence > 0.6 else "LOW RISK"
                else:
                    risk_level = "SAFE" if confidence > 0.8 else "LIKELY SAFE" if confidence > 0.6 else "UNCERTAIN"
                
                all_results[model_name] = {
                    'prediction': prediction,
                    'confidence': confidence,
                    'confidence_percentage': confidence_percentage,
                    'model_name': result['model_name'],
                    'risk_level': risk_level,
                    'available': model_manager._is_model_working(model_name)
                }
                
            except Exception as e:
                logger.warning(f"Model {model_name} failed: {e}")
                all_results[model_name] = {
                    'prediction': 'HAM',
                    'confidence': 0.5,
                    'confidence_percentage': 50.0,
                    'model_name': f"{model_name.upper()} (Not Available)",
                    'risk_level': "UNCERTAIN",
                    'available': False,
                    'error': str(e)
                }
        
        return AllModelsResponse(
            models=all_results,
            message=sms.text.strip(),
            total_models=len(model_names)
        )
        
    except Exception as e:
        logger.error(f"All-models prediction error: {e}")
        raise HTTPException(status_code=500, detail="Internal prediction error")

if __name__ == "__main__":
    import uvicorn
    
    logger.info("üöÄ Starting SMS Spam Detection API...")
    
    try:
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="info"
        )
    except Exception as e:
        logger.error(f"‚ùå Failed to start server: {e}")
        sys.exit(1)